{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to make the demo video \n",
    "\n",
    "\n",
    "1. Perform object detection on the Raspberry Pi 5 using a Hailo model.  \n",
    "2. Convert the `.mp4` video to a `.gif`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai-egg-counter--640x640_quant_hailort_multidevice_1', 'arcface_mobilefacenet--112x112_quant_hailort_hailo8l_1', 'damoyolo_tinynasL35_M--640x640_quant_hailort_hailo8l_1', 'emotion_recognition_fer2013--64x64_quant_hailort_multidevice_1', 'retinaface_mobilenet--736x1280_quant_hailort_hailo8l_1', 'scrfd_10g--640x640_quant_hailort_hailo8l_1', 'scrfd_2.5g--640x640_quant_hailort_hailo8l_1', 'scrfd_500m--640x640_quant_hailort_hailo8l_1', 'yolo11n_pose--640x640_quant_hailort_multidevice_1', 'yolo11n_silu_coco--640x640_quant_hailort_hailo8l_1', 'yolo11s_silu_coco--640x640_quant_hailort_hailo8l_1', 'yolov8n_coco--640x640_quant_hailort_hailo8l_1', 'yolov8n_coco_seg--1280x1280_quant_hailort_hailo8l_1', 'yolov8n_dota_obb--1024x1024_quant_hailort_multidevice_1', 'yolov8n_relu6_age--256x256_quant_hailort_hailo8l_1', 'yolov8n_relu6_car--640x640_quant_hailort_hailo8l_1', 'yolov8n_relu6_coco--640x640_quant_hailort_hailo8l_1', 'yolov8n_relu6_coco_pose--640x640_quant_hailort_hailo8l_1', 'yolov8n_relu6_coco_seg--640x640_quant_hailort_hailo8l_1', 'yolov8n_relu6_face--640x640_quant_hailort_hailo8l_1', 'yolov8n_relu6_fairface_gender--256x256_quant_hailort_hailo8l_1', 'yolov8n_relu6_fire_smoke--640x640_quant_hailort_hailo8l_1', 'yolov8n_relu6_hand--640x640_quant_hailort_hailo8l_1', 'yolov8n_relu6_human_head--640x640_quant_hailort_hailo8l_1', 'yolov8n_relu6_lp--640x640_quant_hailort_hailo8l_1', 'yolov8n_relu6_lp_ocr--256x128_quant_hailort_hailo8l_1', 'yolov8n_relu6_person--640x640_quant_hailort_hailo8l_1', 'yolov8n_relu6_ppe--640x640_quant_hailort_hailo8l_1', 'yolov8n_relu6_widerface_kpts--640x640_quant_hailort_hailo8l_1', 'yolov8n_silu_coco--640x640_quant_hailort_hailo8l_1', 'yolov8s_coco--320x320_quant_hailort_hailo8l_1', 'yolov8s_imagenet--224x224_quant_hailort_hailo8l_1', 'yolov8s_silu_imagenet--224x224_quant_hailort_hailo8l_1']\n"
     ]
    }
   ],
   "source": [
    "import degirum as dg\n",
    "import degirum_tools\n",
    "\n",
    "hailo_model_zoo = dg.connect(\n",
    "    inference_host_address='@local',\n",
    "    zoo_url='degirum/hailo'    \n",
    ")\n",
    "\n",
    "print(hailo_model_zoo.list_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 1\n",
    "- using Yolo 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Video with AI overlay saved at: saved_inference_1.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import degirum as dg\n",
    "import degirum_tools\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 1. Model Configuration\n",
    "# -----------------------------------------------------\n",
    "inference_host_address = \"@local\"\n",
    "zoo_url = \"degirum/hailo\"\n",
    "token = ''\n",
    "device_type = \"HAILORT/HAILO8L\"\n",
    "model_name =  'yolo11n_silu_coco--640x640_quant_hailort_hailo8l_1'\n",
    "\n",
    "# Path to input video\n",
    "video_source = 'Traffic.mp4'\n",
    "# Path to output video (change to your desired location)\n",
    "output_path = 'saved_inference_1.mp4'\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 2. Load Model\n",
    "# -----------------------------------------------------\n",
    "model = dg.load_model(\n",
    "    model_name=model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token,\n",
    "    device_type=device_type\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 3. Prepare VideoWriter (based on input video properties)\n",
    "# -----------------------------------------------------\n",
    "cap = cv2.VideoCapture(video_source)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "cap.release()\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 4. Run inference and save each overlayed frame\n",
    "# -----------------------------------------------------\n",
    "for inference_result in degirum_tools.predict_stream(model, video_source):\n",
    "    # `inference_result.image_overlay` contains the BGR frame with bounding boxes\n",
    "    overlay_frame = inference_result.image_overlay\n",
    "\n",
    "    # Write this overlayed frame to our video file\n",
    "    out.write(overlay_frame)\n",
    "\n",
    "out.release()\n",
    "print(f\" Video with AI overlay saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to ``.gif`` to display on the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ffmpeg -i saved_inference_1.mp4 -filter_complex \"[0:v] fps=10,scale=640:-1:flags=lanczos,split [a][b];[a] palettegen [p];[b][p] paletteuse\" -y demo-1.gif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 2 \n",
    "using Yolo 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Video with AI overlay saved at: saved_inference_2.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import degirum as dg\n",
    "import degirum_tools\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 1. Model Configuration\n",
    "# -----------------------------------------------------\n",
    "inference_host_address = \"@local\"\n",
    "zoo_url = \"degirum/hailo\"\n",
    "token = ''\n",
    "device_type = \"HAILORT/HAILO8L\"\n",
    "model_name =  'yolov8n_relu6_coco--640x640_quant_hailort_hailo8l_1'\n",
    "\n",
    "# Path to input video\n",
    "video_source = 'road_trafifc.mp4'\n",
    "# Path to output video (change to your desired location)\n",
    "output_path = 'saved_inference_2.mp4'\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 2. Load Model\n",
    "# -----------------------------------------------------\n",
    "model = dg.load_model(\n",
    "    model_name=model_name,\n",
    "    inference_host_address=inference_host_address,\n",
    "    zoo_url=zoo_url,\n",
    "    token=token,\n",
    "    device_type=device_type\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 3. Prepare VideoWriter (based on input video properties)\n",
    "# -----------------------------------------------------\n",
    "cap = cv2.VideoCapture(video_source)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "cap.release()\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# 4. Run inference and save each overlayed frame\n",
    "# -----------------------------------------------------\n",
    "for inference_result in degirum_tools.predict_stream(model, video_source):\n",
    "    # `inference_result.image_overlay` contains the BGR frame with bounding boxes\n",
    "    overlay_frame = inference_result.image_overlay\n",
    "\n",
    "    # Write this overlayed frame to our video file\n",
    "    out.write(overlay_frame)\n",
    "\n",
    "out.release()\n",
    "print(f\" Video with AI overlay saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to ``.gif`` to display on the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 5.1.6-0+deb12u1+rpt1 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with gcc 12 (Debian 12.2.0-14)\n",
      "  configuration: --prefix=/usr --extra-version=0+deb12u1+rpt1 --toolchain=hardened --incdir=/usr/include/aarch64-linux-gnu --enable-gpl --disable-stripping --disable-mmal --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libglslang --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librist --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sand --enable-sdl2 --disable-sndio --enable-libjxl --enable-neon --enable-v4l2-request --enable-libudev --enable-epoxy --libdir=/usr/lib/aarch64-linux-gnu --arch=arm64 --enable-pocketsphinx --enable-librsvg --enable-libdc1394 --enable-libdrm --enable-vout-drm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-libplacebo --enable-librav1e --enable-shared\n",
      "  libavutil      57. 28.100 / 57. 28.100\n",
      "  libavcodec     59. 37.100 / 59. 37.100\n",
      "  libavformat    59. 27.100 / 59. 27.100\n",
      "  libavdevice    59.  7.100 / 59.  7.100\n",
      "  libavfilter     8. 44.100 /  8. 44.100\n",
      "  libswscale      6.  7.100 /  6.  7.100\n",
      "  libswresample   4.  7.100 /  4.  7.100\n",
      "  libpostproc    56.  6.100 / 56.  6.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'saved_inference_2.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf59.27.100\n",
      "  Duration: 00:00:10.04, start: 0.000000, bitrate: 6128 kb/s\n",
      "  Stream #0:0[0x1](und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 640x360 [SAR 1:1 DAR 16:9], 6127 kb/s, 29.97 fps, 29.97 tbr, 11988 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:0 (mpeg4) -> fps:default\n",
      "  paletteuse:default -> Stream #0:0 (gif)\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;34m[swscaler @ 0x5555d0342290] \u001b[0m\u001b[1;34m[swscaler @ 0x5555d034ff90] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv420p to bgra.\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x5555d0342290] \u001b[0m\u001b[1;34m[swscaler @ 0x5555d036d410] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv420p to bgra.\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x5555d0342290] \u001b[0m\u001b[1;34m[swscaler @ 0x5555d0389000] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv420p to bgra.\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x5555d0342290] \u001b[0m\u001b[1;34m[swscaler @ 0x5555d03a4bf0] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv420p to bgra.\n",
      "\u001b[0m\u001b[1;34m[swscaler @ 0x5555d0342290] \u001b[0m\u001b[1;34m[swscaler @ 0x5555d03c07e0] \u001b[0m\u001b[0;33mNo accelerated colorspace conversion found from yuv420p to bgra.\n",
      "\u001b[0m\u001b[1;32m[Parsed_palettegen_3 @ 0x5555ce33c370] \u001b[0m255(+1) colors generated out of 323680 colors; ratio=0.000788\n",
      "Output #0, gif, to 'demo-2.gif':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf59.27.100\n",
      "  Stream #0:0: Video: gif, pal8(pc, gbr/unknown/unknown, progressive), 640x360 [SAR 1:1 DAR 16:9], q=2-31, 200 kb/s, 10 fps, 100 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc59.37.100 gif\n",
      "frame=  100 fps= 12 q=-0.0 Lsize=   14683kB time=00:00:09.91 bitrate=12137.7kbits/s speed=1.22x     \n",
      "video:14683kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000133%\n"
     ]
    }
   ],
   "source": [
    "! ffmpeg -i saved_inference_2.mp4 -filter_complex \"[0:v] fps=10,scale=640:-1:flags=lanczos,split [a][b];[a] palettegen [p];[b][p] paletteuse\" -y demo-2.gif\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "degirum_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
